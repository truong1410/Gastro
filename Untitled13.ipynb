{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/truong1410/Gastro/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub13aybh7X4C",
        "outputId": "d0318a8d-e68b-4dc9-fed5-db957aeebd9a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Using device: cpu\n",
            "Number of Training set images: 4904\n",
            "Number of Validation set images: 615\n",
            "Number of Test set images: 619\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 89.5MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Train_loss: 1.396\n",
            "loss:1.396259,micro_precision:0.639274,micro_recall:0.639274,micro_f1:0.639274,macro_precision:0.239251,macro_recall:0.215110,macro_f1:0.220515,mcc:0.570320\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".....\n",
            "Validating...\n",
            "val_loss: 0.830\n",
            "micro_precision:0.767480,micro_recall:0.767480,micro_f1:0.767480,macro_precision:0.485796,macro_recall:0.412688,macro_f1:0.423001,mcc:0.722071\n",
            "val micro f1 increased (0.000000-->0.767480). Saving model\n",
            "----------\n",
            "63m 17s\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Train_loss: 0.818\n",
            "loss:0.818130,micro_precision:0.761215,micro_recall:0.761215,micro_f1:0.761215,macro_precision:0.562770,macro_recall:0.441661,macro_f1:0.457128,mcc:0.714957\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".....\n",
            "Validating...\n",
            "val_loss: 0.743\n",
            "micro_precision:0.790244,micro_recall:0.790244,micro_f1:0.790244,macro_precision:0.482901,macro_recall:0.476435,macro_f1:0.472697,mcc:0.751330\n",
            "val micro f1 increased (0.767480-->0.790244). Saving model\n",
            "----------\n",
            "29m 31s\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Train_loss: 0.718\n",
            "loss:0.718097,micro_precision:0.780383,micro_recall:0.780383,micro_f1:0.780383,macro_precision:0.542701,macro_recall:0.476441,macro_f1:0.494316,mcc:0.738146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".....\n",
            "Validating...\n",
            "val_loss: 0.685\n",
            "micro_precision:0.791870,micro_recall:0.791870,micro_f1:0.791870,macro_precision:0.540021,macro_recall:0.476696,macro_f1:0.489054,mcc:0.752641\n",
            "val micro f1 increased (0.790244-->0.791870). Saving model\n",
            "----------\n",
            "29m 19s\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Train_loss: 0.680\n",
            "loss:0.680272,micro_precision:0.778344,micro_recall:0.778344,micro_f1:0.778344,macro_precision:0.612968,macro_recall:0.481484,macro_f1:0.501725,mcc:0.735855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".....\n",
            "Validating...\n",
            "val_loss: 0.646\n",
            "micro_precision:0.791870,micro_recall:0.791870,micro_f1:0.791870,macro_precision:0.462498,macro_recall:0.454960,macro_f1:0.452653,mcc:0.752057\n",
            "val micro f1 increased (0.791870-->0.791870). Saving model\n",
            "----------\n",
            "29m 31s\n",
            "Epoch 5/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Train_loss: 0.634\n",
            "loss:0.634218,micro_precision:0.795269,micro_recall:0.795269,micro_f1:0.795269,macro_precision:0.639570,macro_recall:0.510668,macro_f1:0.537043,mcc:0.756184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".....\n",
            "Validating...\n",
            "val_loss: 0.634\n",
            "micro_precision:0.790244,micro_recall:0.790244,micro_f1:0.790244,macro_precision:0.547270,macro_recall:0.466118,macro_f1:0.484341,mcc:0.750792\n",
            "----------\n",
            "29m 24s\n",
            "Starting testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[103   0   2   0   0   1   0   0   1   1  17   2   0   0   0]\n",
            " [  0   3   0   0   0   0   0   0   6   0   0   1   0   0   0]\n",
            " [  3   0   7   0   0   1   0   0   0   0   7   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  12   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   2   0   1   0   0]\n",
            " [  2   0   1   0   0   7   0   0   0   2   2   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
            " [  1   2   1   0   0   0   0   0   4   0   0   3   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  29   0   0   3   1   0   0]\n",
            " [  2   0   0   0   0   0   0   0   0   9   9   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 141   4   0   2   0]\n",
            " [  0   0   1   0   0   0   0   0   2   0   0  88   3   3   0]\n",
            " [  0   0   0   0   0   0   0   0   3   0   1  10  26   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   8   3  68   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0]]\n",
            "Confusion matrix saved.\n",
            "Accuracy of the network on the 619 test images: 77.705978 %\n",
            "                                                       precision    recall  f1-score   support\n",
            "\n",
            "                                      Accessory tools       0.93      0.81      0.87       127\n",
            "                                  Barrett's esophagus       0.60      0.30      0.40        10\n",
            "                                       Blood in lumen       0.58      0.39      0.47        18\n",
            "                                                Cecum       0.00      0.00      0.00        12\n",
            "                                    Colon diverticula       0.00      0.00      0.00         3\n",
            "                                    Colorectal cancer       0.78      0.50      0.61        14\n",
            "                                   Esophageal varices       0.00      0.00      0.00         1\n",
            "                                          Esophagitis       0.00      0.00      0.00        11\n",
            "              Gastroesophageal_junction_normal z-line       0.64      0.88      0.74        33\n",
            "                                      Ileocecal valve       0.75      0.45      0.56        20\n",
            "Normal mucosa and vascular pattern in the large bowel       0.72      0.96      0.82       147\n",
            "                                       Normal stomach       0.73      0.91      0.81        97\n",
            "                                              Pylorus       0.76      0.65      0.70        40\n",
            "                           Small bowel_terminal ileum       0.93      0.80      0.86        85\n",
            "                                                Ulcer       0.00      0.00      0.00         1\n",
            "\n",
            "                                             accuracy                           0.78       619\n",
            "                                            macro avg       0.49      0.44      0.46       619\n",
            "                                         weighted avg       0.75      0.78      0.75       619\n",
            "\n",
            "micro_precision:0.777060,micro_recall:0.777060,micro_f1:0.777060,macro_precision:0.494847,macro_recall:0.443007,macro_f1:0.455838,mcc:0.736918\n",
            "Training completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision Pillow scikit-learn matplotlib\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "import os\n",
        "import re\n",
        "import argparse\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import logging\n",
        "import csv\n",
        "from torchvision import transforms, datasets, models\n",
        "import sklearn.metrics as mtc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define paths - modify these to match your Google Drive structure\n",
        "base_path = \"/content/drive/MyDrive/\"  # Change this to your path\n",
        "train_root_dir = os.path.join(base_path, \"Gastro/train\")\n",
        "val_root_dir = os.path.join(base_path, \"Gastro/val\")\n",
        "test_root_dir = os.path.join(base_path, \"Gastro/test\")\n",
        "model_path = os.path.join(base_path, \"checkpoints/\")  # For saving models\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# Parameters (you can adjust these)\n",
        "max_epochs = 5\n",
        "batch_size = 32\n",
        "lr = 0.0005\n",
        "n_classes = 22  # Number of classes\n",
        "\n",
        "# Define filename for saving results\n",
        "filename = f'results_e{max_epochs}_b{batch_size}_lr{lr}_densenet121_improved.csv'\n",
        "\n",
        "# Define transforms\n",
        "trans = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4762, 0.3054, 0.2368], [0.3345, 0.2407, 0.2164])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4762, 0.3054, 0.2368], [0.3345, 0.2407, 0.2164])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4762, 0.3054, 0.2368], [0.3345, 0.2407, 0.2164])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Helper functions (same as before)\n",
        "def print_metrics(metrics, num_steps):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        if k == 'dice_coeff' or k == 'dice' or k == 'bce':\n",
        "            outputs.append('{}:{:4f}'.format(k, metrics[k]/num_steps))\n",
        "        else:\n",
        "            outputs.append('{}:{:2f}'.format(k, metrics[k]))\n",
        "    print('{}'.format(','.join(outputs)))\n",
        "\n",
        "def training_curve(epochs, lossesT, lossesV):\n",
        "    plt.plot(epochs, lossesT, 'c', label='Training loss')\n",
        "    plt.plot(epochs, lossesV, 'm', label='Validation loss')\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig('train_val_epoch_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues, plt_size=[10,10]):\n",
        "    plt.rcParams['figure.figsize'] = plt_size\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "    print(\"Confusion matrix saved.\")\n",
        "\n",
        "def validate_net(model, validation_generator, device, criterion):\n",
        "    num_steps = 0\n",
        "    val_loss = 0\n",
        "    val_metrics = defaultdict(float)\n",
        "    all_labels_d = torch.tensor([], dtype=torch.long).to(device)\n",
        "    all_predictions_d = torch.tensor([], dtype=torch.long).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for image, labels in validation_generator:\n",
        "            image, labels = image.to(device, dtype=torch.float32), labels.to(device)\n",
        "            outputs = model(image)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            num_steps += image.size(0)\n",
        "            val_loss += loss.item() * image.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            all_labels_d = torch.cat((all_labels_d, labels), 0)\n",
        "            all_predictions_d = torch.cat((all_predictions_d, predicted), 0)\n",
        "\n",
        "    y_true = all_labels_d.cpu()\n",
        "    y_predicted = all_predictions_d.cpu()\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_metrics['micro_precision'] = mtc.precision_score(y_true, y_predicted, average=\"micro\")\n",
        "    val_metrics['micro_recall'] = mtc.recall_score(y_true, y_predicted, average=\"micro\")\n",
        "    val_metrics['micro_f1'] = mtc.f1_score(y_true, y_predicted, average=\"micro\")\n",
        "    val_metrics['macro_precision'] = mtc.precision_score(y_true, y_predicted, average=\"macro\")\n",
        "    val_metrics['macro_recall'] = mtc.recall_score(y_true, y_predicted, average=\"macro\")\n",
        "    val_metrics['macro_f1'] = mtc.f1_score(y_true, y_predicted, average=\"macro\")\n",
        "    val_metrics['mcc'] = mtc.matthews_corrcoef(y_true, y_predicted)\n",
        "\n",
        "    return (val_loss/num_steps), val_metrics, num_steps\n",
        "\n",
        "def test_net(model, test_generator, device, criterion):\n",
        "    num_steps = 0\n",
        "    test_loss = 0\n",
        "    test_metrics = defaultdict(float)\n",
        "    all_labels_d = torch.tensor([], dtype=torch.long).to(device)\n",
        "    all_predictions_d = torch.tensor([], dtype=torch.long).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for image, labels in test_generator:\n",
        "            image, labels = image.to(device, dtype=torch.float32), labels.to(device)\n",
        "            outputs = model(image)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            num_steps += image.size(0)\n",
        "            test_loss += loss.item() * image.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            all_labels_d = torch.cat((all_labels_d, labels), 0)\n",
        "            all_predictions_d = torch.cat((all_predictions_d, predicted), 0)\n",
        "\n",
        "    y_true = all_labels_d.cpu()\n",
        "    y_predicted = all_predictions_d.cpu()\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_metrics['micro_precision'] = mtc.precision_score(y_true, y_predicted, average=\"micro\")\n",
        "    test_metrics['micro_recall'] = mtc.recall_score(y_true, y_predicted, average=\"micro\")\n",
        "    test_metrics['micro_f1'] = mtc.f1_score(y_true, y_predicted, average=\"micro\")\n",
        "    test_metrics['macro_precision'] = mtc.precision_score(y_true, y_predicted, average=\"macro\")\n",
        "    test_metrics['macro_recall'] = mtc.recall_score(y_true, y_predicted, average=\"macro\")\n",
        "    test_metrics['macro_f1'] = mtc.f1_score(y_true, y_predicted, average=\"macro\")\n",
        "    test_metrics['mcc'] = mtc.matthews_corrcoef(y_true, y_predicted)\n",
        "\n",
        "    # Confusion matrix and classification report\n",
        "    cm = confusion_matrix(y_true, y_predicted)\n",
        "    class_names = test_generator.dataset.classes\n",
        "    plot_confusion_matrix(cm, classes=class_names, title='Confusion Matrix')\n",
        "\n",
        "    print('Accuracy of the network on the %d test images: %f %%' % (num_steps, (100.0 * (y_predicted == y_true).sum() / num_steps)))\n",
        "    print(classification_report(y_true, y_predicted, target_names=class_names))\n",
        "\n",
        "    return (test_loss/num_steps), test_metrics, num_steps\n",
        "\n",
        "# Main training class with improved model architecture\n",
        "class GastroVisionTrainer:\n",
        "    def __init__(self):\n",
        "        # Create datasets and dataloaders\n",
        "        training_dataset = datasets.ImageFolder(train_root_dir, transform=trans['train'])\n",
        "        validation_dataset = datasets.ImageFolder(val_root_dir, transform=trans['valid'])\n",
        "        test_dataset = datasets.ImageFolder(test_root_dir, transform=trans['test'])\n",
        "\n",
        "        self.training_generator = data.DataLoader(training_dataset, batch_size, shuffle=True)\n",
        "        self.validation_generator = data.DataLoader(validation_dataset, batch_size)\n",
        "        self.test_generator = data.DataLoader(test_dataset, batch_size)\n",
        "\n",
        "        print(f'Number of Training set images: {len(training_dataset)}')\n",
        "        print(f'Number of Validation set images: {len(validation_dataset)}')\n",
        "        print(f'Number of Test set images: {len(test_dataset)}')\n",
        "\n",
        "    def create_improved_model(self):\n",
        "        # Initialize model with proper weights parameter\n",
        "        model = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights.IMAGENET1K_V1).to(device)\n",
        "\n",
        "        # Freeze all layers initially\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Get the number of features from the last layer\n",
        "        num_features = model.classifier.in_features\n",
        "\n",
        "        # Replace classifier with a properly structured sequential model\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Linear(num_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, n_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        ).to(device)\n",
        "\n",
        "        # Unfreeze classifier layers\n",
        "        for param in model.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_net(self):\n",
        "        model = self.create_improved_model()\n",
        "\n",
        "        # Training setup\n",
        "        optimizer = optim.Adam(model.parameters(), lr, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=4, verbose=True)\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        val_f1_max = 0.0\n",
        "        epochs = []\n",
        "        lossesT = []\n",
        "        lossesV = []\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(max_epochs):\n",
        "            print(f'Epoch {epoch+1}/{max_epochs}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            since = time.time()\n",
        "            train_metrics = defaultdict(float)\n",
        "            total_loss = 0\n",
        "            num_steps = 0\n",
        "\n",
        "            all_labels_d = torch.tensor([], dtype=torch.long).to(device)\n",
        "            all_predictions_d = torch.tensor([], dtype=torch.long).to(device)\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            # Training phase\n",
        "            for image, labels in self.training_generator:\n",
        "                image, labels = image.to(device, dtype=torch.float32), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(image)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                num_steps += image.size(0)\n",
        "                total_loss += loss.item() * image.size(0)\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                all_labels_d = torch.cat((all_labels_d, labels), 0)\n",
        "                all_predictions_d = torch.cat((all_predictions_d, predicted), 0)\n",
        "\n",
        "            # Calculate training metrics\n",
        "            y_true = all_labels_d.cpu()\n",
        "            y_predicted = all_predictions_d.cpu()\n",
        "\n",
        "            train_metrics['loss'] = total_loss / num_steps\n",
        "            train_metrics['micro_precision'] = mtc.precision_score(y_true, y_predicted, average=\"micro\")\n",
        "            train_metrics['micro_recall'] = mtc.recall_score(y_true, y_predicted, average=\"micro\")\n",
        "            train_metrics['micro_f1'] = mtc.f1_score(y_true, y_predicted, average=\"micro\")\n",
        "            train_metrics['macro_precision'] = mtc.precision_score(y_true, y_predicted, average=\"macro\")\n",
        "            train_metrics['macro_recall'] = mtc.recall_score(y_true, y_predicted, average=\"macro\")\n",
        "            train_metrics['macro_f1'] = mtc.f1_score(y_true, y_predicted, average=\"macro\")\n",
        "            train_metrics['mcc'] = mtc.matthews_corrcoef(y_true, y_predicted)\n",
        "\n",
        "            print('Training...')\n",
        "            print(f'Train_loss: {train_metrics[\"loss\"]:.3f}')\n",
        "            print_metrics(train_metrics, num_steps)\n",
        "\n",
        "            # Validation phase\n",
        "            val_loss, val_metrics, val_num_steps = validate_net(model, self.validation_generator, device, criterion)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            epochs.append(epoch)\n",
        "            lossesT.append(train_metrics['loss'])\n",
        "            lossesV.append(val_loss)\n",
        "\n",
        "            print('.' * 5)\n",
        "            print('Validating...')\n",
        "            print(f'val_loss: {val_loss:.3f}')\n",
        "            print_metrics(val_metrics, val_num_steps)\n",
        "\n",
        "            # Save results to CSV\n",
        "            self.save_results_to_csv(epoch, train_metrics, val_loss, val_metrics)\n",
        "\n",
        "            # Save best model\n",
        "            if val_metrics['micro_f1'] >= val_f1_max:\n",
        "                print(f'val micro f1 increased ({val_f1_max:.6f}-->{val_metrics[\"micro_f1\"]:.6f}). Saving model')\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler': scheduler.state_dict(),\n",
        "                    'loss': val_loss\n",
        "                }, os.path.join(model_path, f'C_{epoch+1}_{batch_size}_improved.pth'))\n",
        "\n",
        "                val_f1_max = val_metrics['micro_f1']\n",
        "\n",
        "            print('-' * 10)\n",
        "            time_elapsed = time.time() - since\n",
        "            print(f'{time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "\n",
        "        # Plot training curves\n",
        "        training_curve(epochs, lossesT, lossesV)\n",
        "\n",
        "        # Test phase\n",
        "        print(\"Starting testing...\")\n",
        "        test_loss, test_metrics, test_num_steps = test_net(model, self.test_generator, device, criterion)\n",
        "        print_metrics(test_metrics, test_num_steps)\n",
        "\n",
        "        # Save test results\n",
        "        self.save_test_results_to_csv(test_loss, test_metrics)\n",
        "\n",
        "        return val_metrics, test_metrics\n",
        "\n",
        "    def save_results_to_csv(self, epoch, train_metrics, val_loss, val_metrics):\n",
        "        key_name = ['Epoch', 'Train_loss', 'Train_micro_precision', 'Train_micro_recall', 'Train_micro_f1',\n",
        "                   'Train_macro_precision', 'Train_macro_recall', 'Train_macro_f1', 'Train_mcc',\n",
        "                   'Val_loss', 'Val_micro_precision', 'Val_micro_recall', 'Val_micro_f1',\n",
        "                   'Val_macro_precision', 'Val_macro_recall', 'Val_macro_f1', 'Val_mcc']\n",
        "\n",
        "        train_list = [epoch]\n",
        "        train_list.extend([train_metrics[k] for k in ['loss', 'micro_precision', 'micro_recall', 'micro_f1',\n",
        "                                                    'macro_precision', 'macro_recall', 'macro_f1', 'mcc']])\n",
        "        train_list.append(val_loss)\n",
        "        train_list.extend([val_metrics[k] for k in ['micro_precision', 'micro_recall', 'micro_f1',\n",
        "                                                   'macro_precision', 'macro_recall', 'macro_f1', 'mcc']])\n",
        "\n",
        "        try:\n",
        "            with open(os.path.join(base_path, filename), 'a', newline=\"\") as f:\n",
        "                wr = csv.writer(f, delimiter=\",\")\n",
        "                if epoch == 0:\n",
        "                    wr.writerow(key_name)\n",
        "                wr.writerow(train_list)\n",
        "        except IOError as e:\n",
        "            print(f\"I/O Error: {e}\")\n",
        "\n",
        "    def save_test_results_to_csv(self, test_loss, test_metrics):\n",
        "        key_name = ['Test_loss', 'Test_micro_precision', 'Test_micro_recall', 'Test_micro_f1',\n",
        "                   'Test_macro_precision', 'Test_macro_recall', 'Test_macro_f1', 'Test_mcc']\n",
        "\n",
        "        test_list = [test_loss]\n",
        "        test_list.extend([test_metrics[k] for k in ['micro_precision', 'micro_recall', 'micro_f1',\n",
        "                                                  'macro_precision', 'macro_recall', 'macro_f1', 'mcc']])\n",
        "\n",
        "        try:\n",
        "            with open(os.path.join(base_path, filename), 'a', newline=\"\") as f:\n",
        "                wr = csv.writer(f, delimiter=\",\")\n",
        "                wr.writerow([])\n",
        "                wr.writerow(key_name)\n",
        "                wr.writerow(test_list)\n",
        "        except IOError as e:\n",
        "            print(f\"I/O Error: {e}\")\n",
        "\n",
        "# Run the training\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "    logging.info(f'''Starting training:\n",
        "        Epochs: {max_epochs}\n",
        "        Batch Size: {batch_size}\n",
        "        Learning Rate: {lr}''')\n",
        "\n",
        "    trainer = GastroVisionTrainer()\n",
        "    val_metrics, test_metrics = trainer.train_net()\n",
        "\n",
        "    print(\"Training completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdHWjvhAtTcHGHaqiKMUnt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}